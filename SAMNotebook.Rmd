---
title: "Stochastic Antecedent Modelling Notebook"
author: "Jon Page"
Last Updated: date()
output:
  html_document:
    df_print: paged
Date: 16/03/2020
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```


***
# Introduction

This R Markdown Notebook replicates and expands on the work of Ogle et al in 
their 2015 paper.

Ogle, K., Barber, J.J., Barron-Gafford, G.A., Bentley, L.P., Young, J.M., 
Huxman, T.E., Loik, M.E., Tissue, D.T., 2015. 
Quantifying ecological memory in plant and ecosystem processes. 
Ecol Lett 18, 221â€“235. https://doi.org/10.1111/ele.12399

***
# Model Construction

The below code is mostly a re-working of Appendix S2 from Ogle et al. 
The initial step was reproducing the model and results from the paper.
The below section details this.

First we call the model wrapper. The following inputs must be specified:

Datasets - the csv files that provide the ANPP and precip data
Nlag - the number of years for which antecedent precip is assumed to impact NPP
block - the the partitioning of the months into time blocks
Model run parameters - samples, burn, nadapt, nchain, thin
parameters - the parameters in the model that we want to track

```{r}

## Data Initialisation and Model Set Up

rm(list=ls()) # Clear workspace
graphics.off() # This closes all of R's graphics windows.
cat("\f") # Clear console

library(rstudioapi) # Source rstudioapi to set working directory as needed
library(rjags) # Source rjags for Bayesian analysis
library(ggplot2) # Source ggplot2 to plot results
library(gridExtra) # Source gridExtra for better plots

setwd(getwd())

# First import the data (csv files stolen from De Kauwe - data is provided in 
# pdf form in Ogle 2015)

# Import the ANPP and precip partitioned by event data
ANPPandPrecip = read.table("data/dataset2.csv", 
                           header = TRUE, 
                           stringsAsFactors = FALSE)
    # Note that these data are total annual precip (mm) 
    # partitioned into the size of the events that produced the precip:
    # Event1 = received in rain events with <5mm precip
    # Event2 = received in rain events with 5-15mm precip
    # Event3 = received in rain events with 15-30mm precip 
    # Event4 = received in rain events with >30mm precip
    # NPP is in g/m^2. It is actually the forage produced which in Lauenroth 
    # and Sala (1992) is shown to be linearly correlated with ANPP

# Import monthly precip data
Precip = read.table("data/dataset3.csv", 
                    header = TRUE, 
                    stringsAsFactors = FALSE)
    # This covers 91 years
    # Note the precip totals are in inches - the model converts this to mm

# Specify the investigated lag length and the assignment of months to time 
# blocks
# # The number of previous years for which precipitation may affect current NPP
'Nlag'=5 
# Partition months into time blocks e.g. for month 10,11,12 in year 5, 
# group them into time block 38
'block'= matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 
                           16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 25, 26, 26, 
                           27, 27, 28, 28, 29, 29, 30, 30, 31, 31, 31, 32, 32, 
                           32, 33, 33, 33, 34, 34, 34, 35, 35, 35, 36, 36, 36, 
                           37, 37, 37, 38, 38, 38), 
                    nrow=Nlag,ncol=12,byrow = TRUE)

# makes more sense if you visualise it...
block

# Define the parameters for the model operation
# samples to be kept after burn in
samples <- 50000 
# iterations for burn in
burn <- samples * 0.1 
# number of iterations where samplers adapt behaviour to maximise efficiency
nadapt <- 100  
# The number of MCMC chains to run
nchains <- 4 
# thinning rate
# save every thin-th iteration to reduce correlation between 
# consecutive values in the chain
thin <- 10 

# Decide the variables to track
parameters = c('mu','a','weightOrdered','cum.weight','sumD1','weight') 

# Combine the data required for the model
Data = list('Nlag'=Nlag 
            ,'block'= block
            # number of years for which NPP and precip event data are available
            ,'N'=nrow(ANPPandPrecip) 
            # number of years for which monthly precipitation data is available
            ,'Nyrs'=nrow(Precip) 
            # number of time blocks the months are partitioned into
            ,'Nblocks'=max(block) 
            # Monthly precip data
            ,'ppt'=Precip[,-1] 
            # Year ID for NPP
            ,'YearID'=ANPPandPrecip$YearID 
            # Yearly precip event data
            ,'Event'=ANPPandPrecip[,c(4,5,6,7)] 
            # Yearly NPP data - comment this out to obtain the priors
#            ,'NPP'=ANPPandPrecip[,2] 
            )

# Put the model system into a variable
jags <- jags.model('Model.R', data=Data, n.chains=nchains, n.adapt=nadapt) 

```

Now we can run the model. Note, we might not need to if the output files already
exist!!

```{r}
# Generate the MCMC chain (this is basically running the Bayesian analysis)
fit <- coda.samples(jags, n.iter=samples, n.burnin=burn, thin=thin,
                    variable.names=parameters)

# Save the summary of the output as either the prior or posterior
# if NPP data isn't used in the model runs 
# then the output is the prior distributions
 if (length(Data$NPP)==0){ 
      priorSummary = summary(fit)
      save("priorSummary",file="priorSummary.Rdata")
 } else{ 
# NPP data is specified and the output is the posterior distributions
      posteriorSummary = summary(fit)
      save("posteriorSummary",file="posteriorSummary.Rdata")
   }
```

***

Now that we have modelled NPP based on precipitation using Bayesian methods, we
can perform some analysis to confirm that the model has behaved similarly to that
from Ogle et al (it should have done because we used their code!)

We now want to plot figure 2 from Ogle et al for the NPP model to compare our
results to theirs.

```{r}

# Yearly NPP data
NPPobserved=ANPPandPrecip[,2] 
  
# Load the prior and posterior data
load("priorSummary.RData")
load("posteriorSummary.Rdata")

# Create variables for the parameters of interest
priStats = data.frame(priorSummary$statistics)
priQntls = data.frame(priorSummary$quantiles)

priMu = priStats[grep("mu",row.names(priStats)),]
priA = priStats[grep("a",row.names(priStats)),]
priCum.weight = priStats[grep("cum.weight",row.names(priStats)),]
priCum.weightQntls = priQntls[grep("cum.weight",row.names(priQntls)),]
priSumD1 = priStats[grep("sumD1",row.names(priStats)),]
priSumD1Qntls = priQntls[grep("sumD1",row.names(priStats)),]

posStats = data.frame(posteriorSummary$statistics)
posQntls = data.frame(posteriorSummary$quantiles)

posMu = posStats[grep("mu",row.names(posStats)),]
posMuQntls = posQntls[grep("mu",row.names(posStats)),]
posA = posStats[grep("a",row.names(posStats)),]
posAQntls = posQntls[grep("a",row.names(posStats)),]
posCum.weight = posStats[grep("cum.weight",row.names(posStats)),]
posCum.weightQntls = posQntls[grep("cum.weight",row.names(posQntls)),]
posSumD1 = posStats[grep("sumD1",row.names(posStats)),]
posSumD1Qntls = posQntls[grep("sumD1",row.names(posStats)),]


# Create data frames for these variables to facilitate plotting  
posYearlyWeights = data.frame(YearIntoPast = 0:4, 
                              Weight = posSumD1$Mean/sum(posSumD1$Mean), 
                              min = posSumD1Qntls$X2.5./sum(posSumD1$Mean), 
                              max = posSumD1Qntls$X97.5./sum(posSumD1$Mean))
priYearlyWeights = data.frame(YearIntoPast = 0:4, 
                              Weight = priSumD1$Mean/sum(priSumD1$Mean), 
                              min = priSumD1Qntls$X2.5./sum(priSumD1$Mean), 
                              max = priSumD1Qntls$X97.5./sum(priSumD1$Mean))

# Define the corresponding variables for the alpha parameters
aDefinitions=factor(c("PPT","E_0-5","E_5-15","E_15-30","E_>30"),
                    levels=c("PPT","E_0-5","E_5-15","E_15-30","E_>30")) 

posYearlyA = data.frame(aDefinitions, 
                              Covariates = posA$Mean[2:6], 
                              min = posAQntls$X2.5.[2:6], 
                              max = posAQntls$X97.5.[2:6])

# Replicate the plot from Ogle et al 2015 
# for the alpha and yearly weight (page 227)
plot1 <- ggplot(posYearlyWeights,aes(YearIntoPast,
                                     Weight,
                                     ymin = min, 
                                     ymax = max)) + 
      geom_ribbon(data=priYearlyWeights,fill="grey70") +
      geom_line(data=priYearlyWeights) +
      geom_pointrange(data=posYearlyWeights) +
      ylim(0,1)

plot2 <- ggplot(posYearlyWeights,aes(aDefinitions,
                                     Covariates,
                                     ymin = min, 
                                     ymax = max)) + 
      geom_pointrange(data=posYearlyA)
 

grid.arrange(plot1, plot2, nrow = 1)

```

The right hand graph is very similar to Ogle et al - note that the scale of our
y axis is fixed while Ogle et al's changes at y=1.

The left hand graph is very similar to Ogle et al when looking at the posteriors
and also the mean of the priors. However, the credibility intervals (CI) for the
prior do not increase for years 3 and 4 as seen in Ogle et al. 

Ogle's lecture notes state that the 2.5 to 97.5 percentile specifies a CI which
is what is used here. I am unsure why Ogle's CIs expand and ours don't. 

**The above comments are no longer relevant. block was being assigned incorrectly
and since this has been fixed, the above issue is resolved. Our graph matches 
that from the paper, insofar as stochastic methods will allow.**

***

We now plot the observed NPP against our modelled NPP

```{r}
# Plot modelled NPP against observed NPP

NPPobs = data.frame(Year=1:nrow(ANPPandPrecip)+1938,
                    NPP_obs = NPPobserved)
NPPmod = data.frame(Year=1:nrow(ANPPandPrecip)+1938,
                    NPP_mod = posMu[,1],
                    NPP_modmin = posMuQntls[,1],
                    NPP_modmax = posMuQntls[,5])

plot3 <- ggplot(NPPobs) +
  geom_line(data=NPPobs,aes(Year,NPP_obs),color='steelblue',size=2) +
  geom_point(data=NPPobs,aes(Year,NPP_obs),color='steelblue',size=2,na.rm=TRUE) +
  geom_ribbon(data=NPPmod, aes(x=Year, ymin=NPP_modmin, ymax=NPP_modmax), fill="grey70", alpha=0.4) +
  geom_line(data=NPPmod,aes(Year,NPP_mod)) +
  theme_bw() +
  theme(axis.line=element_line(colour = "black"),
    panel.grid.major=element_blank(),
    panel.grid.minor=element_blank(),
    panel.border=element_blank(),
    panel.background=element_blank()) 
  

grid.arrange(plot3)
```


The blue line is the observed NPP. We can see that for the vast majority of years
the observed NPP falls within the CI of our model output. 2 years do not have 
observed NPP which is why we have gaps in the graph.


***

We also replicate figure 4a from Ogle et al. This is the cumulative monthly 
weights.

```{r}
posCum.weightdf = data.frame(monthInPast=1:nrow(posCum.weight),
                    CumulativeWeight = posCum.weight[,1],
                    min = posCum.weightQntls[,1],
                    max = posCum.weightQntls[,5])
priCum.weightdf = data.frame(monthInPast=1:nrow(priCum.weight),
                             CumulativeWeight = priCum.weight[,1],
                             min = priCum.weightQntls[,1],
                             max = priCum.weightQntls[,5])


plot4 <- ggplot(posCum.weightdf) +
      geom_ribbon(data=priCum.weightdf,
                  aes(monthInPast,ymin=min,ymax=max),
                  fill='grey70') +
      coord_cartesian(ylim=c(-0.015,1.015),xlim=c(0,60.5),expand = FALSE) +
      geom_vline(xintercept = 12,color='grey40') +
      geom_vline(xintercept = min(posCum.weightdf$monthInPast[posCum.weightdf$max>=0.9]),
                 color='grey40') +
      geom_vline(xintercept = min(posCum.weightdf$monthInPast[posCum.weightdf$CumulativeWeight>=0.9]),
                 linetype = 'dotted') +
      geom_pointrange(data=posCum.weightdf,aes(monthInPast,
                                          CumulativeWeight,
                                          ymin=min,
                                          ymax=max)) +
      geom_line(data=priCum.weightdf,aes(monthInPast, CumulativeWeight)) +
      geom_hline(yintercept = 0.9,
                 linetype = 'dashed') 


grid.arrange(plot4)
```
We can see how similar our plot is to that of Ogle et al's. 

Of note is that the weight where c = 0.9 is 3 months higher than Ogle's 
(53 vs 50) - therefore our result appears to show that the memory of precipitation
is longer. 
Additional the point at which c = 0.9 is first within the CI of a weight is 48 
for us, while Ogle has this at 42.

Combined with the mismatching priors in Figure 2, this is concerning.

**The above comments are no longer relevant. block was being assigned incorrectly
and since this has been fixed, the above issue is resolved. Our graph matches 
that from the paper, insofar as stochastic methods will allow.**


*** 

A further check is calculating the R^2 value for our model, and compare this to
the value specified by Ogle et al of 0.75 (Table 1).

```{r}
# Calculate value of R2
RSS = sum((posMu[,1]-NPPobserved)^2,na.rm=TRUE)
TSS = sum((NPPobserved-mean(NPPobserved,na.rm=TRUE))^2,na.rm=TRUE)
R2=1-(RSS)/(TSS)
print(R2)
```
As we can see our result is very similar and it is unclear exactly how Ogle et al
calculated their R^2.

***
# Initial investigations

Now that the model is up and running, we can start investigating how changing
the lag or time blocking affects our results.

Two more basic model runs were produced - one with 5 year lag but 60 time blocks
(i.e. each month had a unique weight) and one with a 1 year lag and 12 time 
blocks.

Attempts to plot the results of modelled NPP vs observed are currently struggling
to offset the error bars for clarity...

Therefore, we shall just compare R2 values for now

```{r}
# # Define a function that loads our model output files since all outputs have the
# same variable name
loadRData = function(filename){
      load(filename)
      filename <- get(ls()[grep("Summary",ls())])
      }

# Load the model outputs
priorlag5block38 <- loadRData("priorSummary.Rdata")
posteriorlag5block38 <- loadRData("posteriorSummary.Rdata")
priorlag1block12 <- loadRData("priorSummary_1lag_12blocks.Rdata")
posteriorlag1block12 <- loadRData("posteriorSummary_1lag_12blocks.Rdata")
priorlag5block60 <- loadRData("priorSummary_5lag_60blocks.Rdata")
posteriorlag5block60 <- loadRData("posteriorSummary_5lag_60blocks.Rdata")

# Extract the statistics and the quantiles
Stats = list("pri5.38"=priorlag5block38$statistics,
             "pri5.60"=priorlag5block60$statistics,
             "pri1.12"=priorlag1block12$statistics,
             "pos5.38"=posteriorlag5block38$statistics,
             "pos5.60"=posteriorlag5block60$statistics,
             "pos1.12"=posteriorlag1block12$statistics)

Quantiles = list("pri5.38"=priorlag5block38$quantiles,
                 "pri5.60"=priorlag5block60$quantiles,
                 "pri1.12"=priorlag1block12$quantiles,
                 "pos5.38"=posteriorlag5block38$quantiles,
                 "pos5.60"=posteriorlag5block60$quantiles,
                 "pos1.12"=posteriorlag1block12$quantiles)

# Extract the modelled NPP
NPP5.38 = data.frame("Year"=1:52+1938,
                     "mean"=Stats$pos5.38[grep("mu",row.names(Stats$pos5.38)),1],
                     "min"=Quantiles$pos5.38[grep("mu",row.names(Quantiles$pos5.38)),1],
                     "max"=Quantiles$pos5.38[grep("mu",row.names(Quantiles$pos5.38)),5])
NPP5.60 = data.frame("Year"=1:52+1938,
                     "mean"=Stats$pos5.60[grep("mu",row.names(Stats$pos5.60)),1],
                     "min"=Quantiles$pos5.60[grep("mu",row.names(Quantiles$pos5.60)),1],
                     "max"=Quantiles$pos5.60[grep("mu",row.names(Quantiles$pos5.60)),5])
NPP1.12 = data.frame("Year"=1:52+1938,
                     "mean"=Stats$pos1.12[grep("mu",row.names(Stats$pos1.12)),1],
                     "min"=Quantiles$pos1.12[grep("mu",row.names(Quantiles$pos1.12)),1],
                     "max"=Quantiles$pos1.12[grep("mu",row.names(Quantiles$pos1.12)),5])

#Load ANPP and Precip data and place into dataframe
ANPPandPrecip = read.table("data/dataset2.csv", header = TRUE, 
                           stringsAsFactors = FALSE)
NPPobserved=data.frame("Year" = 1:52+1938,"NPP"=ANPPandPrecip[,2])

RSS5.38 = sum((NPP5.38$mean-NPPobserved$NPP)^2,na.rm=TRUE)
RSS5.60 = sum((NPP5.60$mean-NPPobserved$NPP)^2,na.rm=TRUE)
RSS1.12 = sum((NPP1.12$mean-NPPobserved$NPP)^2,na.rm=TRUE)
TSS = sum((NPPobserved$NPP-mean(NPPobserved$NPP,na.rm=TRUE))^2,na.rm=TRUE)
R2_5.38=1-(RSS5.38)/(TSS)
R2_5.60=1-(RSS5.60)/(TSS)
R2_1.12=1-(RSS1.12)/(TSS)

print(paste("Lag of 5 years and 38 time blocks gives R2 = ", round(R2_5.38,3), sep = ""))
print(paste("Lag of 5 years and 60 time blocks gives R2 = ", round(R2_5.60,3), sep = ""))
print(paste("Lag of 1 year and 12 time blocks gives R2 = ", round(R2_1.12,3), sep = ""))
```

As can be seen, looking only at 1 year gives a substantially worse fit.
Meanwhile, having 60 timeblocks rather than 38 only marginally improves the fit
while dramatically increasing the model runtime. Calculating the DIC would 
probably show a significant preference for the 5.38 run.


*** 

# Creating a function

In an attempt to automate more of the modelling process, I have written a function
that performs the job of the old wrapper script.

I have also written a function that produces Nlag and block if we input the 
number of years that are grouped into each of 1,2,3,6,and 12 monthly weights.
Ideally this function would provide greater flexibility.

I have also written a little script that produces every permutation of assigning
0,1 or 2 years to each of the categories. This would give us 243 different
categorisations to run the model for - this is probably a bit optimistic. 

However, running these and comparing model outputs may well show us something
about lag times. Namely we will have tested lags of length 1 year up to 10 years.
We will also have tested internal weighting of months.


```{r}
# Source the function
source("SAMFunction.R")

# Create a grid of permutations of 5 values either 0, 1 or 2
t = expand.grid(rep(list(0:1), 5))
# Turn this into a matrix for easier extraction
p = matrix(c(t$Var1,t$Var2,t$Var3,t$Var4,t$Var5),nrow = length(t$Var1))
# Print the head of t to visualise this
print(head(t))

# For each row of p but the first (which has a lag of 0), 
# extract the Yj values and run timeblock
for (i in 2:nrow(p)){
  for (j in 1:5){
    assign(paste("Y",j,sep=""), p[i,j])  
  }
 timeblock = timeblocks(Y1,Y2,Y3,Y4,Y5)
 Nlag = timeblock$Nlag
 block = timeblock$block
 # For the timeblock created, run the model
 SAM("data/dataset2.csv","data/dataset3.csv",Nlag,block,prior=FALSE)
}

```

***

# Model Comparison

Having spoken to Manon, we want to try and rank each model by a quantile rank
method.

First we put all the available outputs into a dataframe

```{r}
# Find all outputs available
models = list.files(pattern = "SAM_posterior")
# Load them
for (i in 1:length(k)){
  load(k[i])
}
# Find all the model outputs we've loaded
models = objects(pattern = "SAM_posterior")

get(models[1])$NMSE

dd = data.frame(t(sapply(SAM_posterior_1_1,c)))

MODELS = data.frame(row.names=models)

for (i in 1:length(models)){
MODELS[i,1:11]=data.frame(t(sapply(get(models[i]),c)))
}


# The metrics are as such
# Minimise: DIC, MAE, Q5, Q95, NMSE
# Maximise: R2
```



** To do:
+ Produce a script/function that compares model outputs by quantile ranking the 
model performance metrics
+ Create a function that produces a "block" that groups weight as "spring" rain 
and non-spring rain
+ Create a function that, for a specific length of time N, creates an ensemble of
models where a model exists for every possible block of N years of observed data
that has been fed only those years of data and then assessed OOS against the other
blocks of N years. It will be interesting to model on eg. 1965-1975 with a lag of 
5 years and then see how model performance compares for 1950-1960 and 1980-1990

